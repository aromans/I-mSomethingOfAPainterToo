{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"*ProGAN Code by Aladdin Persson https://www.youtube.com/watch?v=nkQHASviYac*","metadata":{}},{"cell_type":"code","source":"# Model Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom math import log2\n\n# Util Imports\nimport random\nimport numpy as np\nimport os\n\nimport torchvision\nfrom torchvision.utils import save_image\n\n# Train Imports\nimport wandb\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:00.772472Z","iopub.execute_input":"2022-02-17T19:24:00.772776Z","iopub.status.idle":"2022-02-17T19:24:00.781144Z","shell.execute_reply.started":"2022-02-17T19:24:00.772725Z","shell.execute_reply":"2022-02-17T19:24:00.780469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='ImAProPainter')\nrun_id = wandb.run.id","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:00.782419Z","iopub.execute_input":"2022-02-17T19:24:00.782851Z","iopub.status.idle":"2022-02-17T19:24:32.279323Z","shell.execute_reply.started":"2022-02-17T19:24:00.782816Z","shell.execute_reply":"2022-02-17T19:24:32.278635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:32.281399Z","iopub.execute_input":"2022-02-17T19:24:32.28168Z","iopub.status.idle":"2022-02-17T19:24:32.287103Z","shell.execute_reply.started":"2022-02-17T19:24:32.28164Z","shell.execute_reply":"2022-02-17T19:24:32.285945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WSConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n        \n        # initialize the conv layer\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n        \n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:32.288306Z","iopub.execute_input":"2022-02-17T19:24:32.288701Z","iopub.status.idle":"2022-02-17T19:24:32.300239Z","shell.execute_reply.started":"2022-02-17T19:24:32.288666Z","shell.execute_reply":"2022-02-17T19:24:32.299545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PixelNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.epsilon = 1e-8\n        \n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:35.10935Z","iopub.execute_input":"2022-02-17T19:24:35.109681Z","iopub.status.idle":"2022-02-17T19:24:35.117099Z","shell.execute_reply.started":"2022-02-17T19:24:35.109643Z","shell.execute_reply":"2022-02-17T19:24:35.116242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n        super().__init__()\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2)\n        self.pn = PixelNorm()\n        self.use_pn = use_pixelnorm\n        \n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.pn(x) if self.use_pn else x\n        x = self.leaky(self.conv2(x))\n        x = self.pn(x) if self.use_pn else x\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:35.54155Z","iopub.execute_input":"2022-02-17T19:24:35.541986Z","iopub.status.idle":"2022-02-17T19:24:35.550426Z","shell.execute_reply.started":"2022-02-17T19:24:35.541943Z","shell.execute_reply":"2022-02-17T19:24:35.549568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim, in_channels, img_channels=3):\n        super().__init__()\n        self.initial = nn.Sequential(\n            PixelNorm(),\n            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), #1x1 -> 4x4\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            PixelNorm()\n        )\n        \n        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n        \n        for i in range(len(factors) - 1):\n            # factors[i] -> factors[i+1]\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i+1])\n            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n    \n    def fade_in(self, alpha, upscaled, generated):\n        return torch.tanh(alpha * generated + (1-alpha) * upscaled)\n    \n    def forward(self, x, alpha, steps): #steps=0 (4x4), steps=1 (8x8), ....\n        out = self.initial(x) # 4x4\n        \n        if steps == 0:\n            return self.initial_rgb(out)\n        \n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n            out = self.prog_blocks[step](upscaled)\n            \n        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n        final_out = self.rgb_layers[steps](out)\n        return self.fade_in(alpha, final_upscaled, final_out)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:35.897344Z","iopub.execute_input":"2022-02-17T19:24:35.897609Z","iopub.status.idle":"2022-02-17T19:24:35.914163Z","shell.execute_reply.started":"2022-02-17T19:24:35.897562Z","shell.execute_reply":"2022-02-17T19:24:35.913517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels, img_channels=3):\n        super().__init__()\n        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n        self.leaky = nn.LeakyReLU(0.2)\n        \n        for i in range(len(factors) - 1, 0, -1):\n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i-1])\n            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n        \n        # Mirrors the 4x4 img resolution from generator\n        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.rgb_layers.append(self.initial_rgb)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        \n        # block for 4x4 resolution\n        self.final_block = nn.Sequential(\n            WSConv2d(in_channels+1, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, 1, kernel_size=1, stride=1, padding=0)\n        )\n    \n    def fade_in(self, alpha, downscaled, out):\n        return alpha * out + (1 - alpha) * downscaled\n    \n    def minibatch_std(self, x):\n        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        return torch.cat([x, batch_statistics], dim=1) # 512 -> 513\n    \n    def forward(self, x, alpha, steps): # steps=0 (4x4), steps=1 (8x8), ....\n        cur_step = len(self.prog_blocks) - steps\n        out = self.leaky(self.rgb_layers[cur_step](x))\n        \n        if steps == 0:\n            out = self.minibatch_std(out)\n            return self.final_block(out).view(out.shape[0], -1)\n        \n        downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n        out = self.avg_pool(self.prog_blocks[cur_step](out))\n        out = self.fade_in(alpha, downscaled, out)\n        \n        for step in range(cur_step + 1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avg_pool(out)\n            \n        out = self.minibatch_std(out)\n        return self.final_block(out).view(out.shape[0], -1)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:36.22781Z","iopub.execute_input":"2022-02-17T19:24:36.228105Z","iopub.status.idle":"2022-02-17T19:24:36.250186Z","shell.execute_reply.started":"2022-02-17T19:24:36.228063Z","shell.execute_reply":"2022-02-17T19:24:36.248255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparams","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/images\n!cp -r /kaggle/input/gan-getting-started/monet_jpg /kaggle/working/images\n!cp -r /kaggle/input/proganmonet/generator.pth /kaggle/working/\n!cp -r /kaggle/input/proganmonet/critic.pth /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:37.521609Z","iopub.execute_input":"2022-02-17T19:24:37.522101Z","iopub.status.idle":"2022-02-17T19:24:43.688073Z","shell.execute_reply.started":"2022-02-17T19:24:37.522053Z","shell.execute_reply":"2022-02-17T19:24:43.686985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START_TRAIN_AT_IMG_SIZE = 16\nDATASET = \"/kaggle/working/images/\"\nCHECKPOINT_GEN = \"generator.pth\"\nCHECKPOINT_CRITIC = \"critic.pth\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSAVE_MODEL = True\nLOAD_MODEL = True\nLEARNING_RATE = 1e-3\nBATCH_SIZES = [16, 16, 16, 16, 16, 16, 16, 8, 4]\nIMAGE_SIZE = 256\nCHANNELS_IMG = 3\nZ_DIM = 256\nIN_CHANNELS = 256\nLAMBDA_GP = 10\nNUM_STEPS = int(log2(IMAGE_SIZE / 4)) + 1\n\nPROGRESSIVE_EPOCHS = [100] * len(BATCH_SIZES)\nFIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\nNUM_WORKERS = 4","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:43.691927Z","iopub.execute_input":"2022-02-17T19:24:43.692151Z","iopub.status.idle":"2022-02-17T19:24:46.421735Z","shell.execute_reply.started":"2022-02-17T19:24:43.692121Z","shell.execute_reply":"2022-02-17T19:24:46.420824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"def plot_to_wandb(loss_critic, loss_gen, real, fake):\n    wandb.log({\n        \"Loss Critic\": loss_critic,\n        \"Loss Generator\": loss_gen,\n    })\n    \n    with torch.no_grad():\n        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n        wandb.log({\n            \"Real Images\": [wandb.Image(i) for i in img_grid_real],\n            \"Fake Images\": [wandb.Image(i) for i in img_grid_fake]\n        })","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:47.574209Z","iopub.execute_input":"2022-02-17T19:24:47.574694Z","iopub.status.idle":"2022-02-17T19:24:47.581795Z","shell.execute_reply.started":"2022-02-17T19:24:47.574658Z","shell.execute_reply":"2022-02-17T19:24:47.581079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n    batch_size, c, h, w = real.shape\n    beta = torch.rand((batch_size, 1, 1, 1)).repeat(1, c, h, w).to(device)\n    \n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n    \n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n    \n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:48.182293Z","iopub.execute_input":"2022-02-17T19:24:48.182524Z","iopub.status.idle":"2022-02-17T19:24:48.191136Z","shell.execute_reply.started":"2022-02-17T19:24:48.182496Z","shell.execute_reply":"2022-02-17T19:24:48.189949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving Checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict()\n    }\n    torch.save(checkpoint, filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:48.81352Z","iopub.execute_input":"2022-02-17T19:24:48.813845Z","iopub.status.idle":"2022-02-17T19:24:48.820681Z","shell.execute_reply.started":"2022-02-17T19:24:48.813808Z","shell.execute_reply":"2022-02-17T19:24:48.819949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading Checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    \n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:49.608504Z","iopub.execute_input":"2022-02-17T19:24:49.609043Z","iopub.status.idle":"2022-02-17T19:24:49.614928Z","shell.execute_reply.started":"2022-02-17T19:24:49.609007Z","shell.execute_reply":"2022-02-17T19:24:49.61421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_examples(gen, steps, n=100):\n    gen.eval()\n    alpha = 1.0\n    for i in range(n):\n        with torch.no_grad():\n            noise = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)\n            img = gen(noise, alpha, steps)\n            save_image(img * 0.5 + 0.5, f\"saved_examples/img_{i}.png\")\n            \n    gen.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:50.075966Z","iopub.execute_input":"2022-02-17T19:24:50.0762Z","iopub.status.idle":"2022-02-17T19:24:50.086547Z","shell.execute_reply.started":"2022-02-17T19:24:50.076173Z","shell.execute_reply":"2022-02-17T19:24:50.085648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"torch.backends.cudnn.benchmarks = True # performance benefits!","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:51.008796Z","iopub.execute_input":"2022-02-17T19:24:51.009268Z","iopub.status.idle":"2022-02-17T19:24:51.015499Z","shell.execute_reply.started":"2022-02-17T19:24:51.009233Z","shell.execute_reply":"2022-02-17T19:24:51.01464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loader(image_size):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Normalize(\n                [0.5 for _ in range(CHANNELS_IMG)],\n                [0.5 for _ in range(CHANNELS_IMG)]\n            ),\n        ]\n    )\n    \n    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:51.441967Z","iopub.execute_input":"2022-02-17T19:24:51.442193Z","iopub.status.idle":"2022-02-17T19:24:51.451696Z","shell.execute_reply.started":"2022-02-17T19:24:51.442166Z","shell.execute_reply":"2022-02-17T19:24:51.450991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(\n    critic,\n    gen,\n    loader,\n    dataset,\n    step,\n    alpha,\n    opt_critic,\n    opt_gen,\n    scalar_gen,\n    scalar_critic\n):\n    loop = tqdm(loader, leave=True)\n    for batch_idx, (real, _) in enumerate(loop):\n        real = real.to(DEVICE)\n        cur_batch_size = real.shape[0]\n        \n        # Train Critic: max E[critic(real) - E[critic(fake)]]\n        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n        \n        with torch.cuda.amp.autocast():\n            fake = gen(noise, alpha, step)\n            critic_real = critic(real, alpha, step)\n            critic_fake = critic(fake.detach(), alpha, step)\n            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n            loss_critic = (\n                -(torch.mean(critic_real) - torch.mean(critic_fake))\n                + LAMBDA_GP * gp\n                + (0.001 * torch.mean(critic_real ** 2))\n            )\n            \n        opt_critic.zero_grad()\n        scalar_critic.scale(loss_critic).backward()\n        scalar_critic.step(opt_critic)\n        scalar_critic.update()\n        \n        # Train Generator: max E[critic(gen_fake)]\n        with torch.cuda.amp.autocast():\n            gen_fake = critic(fake, alpha, step)\n            loss_gen = -torch.mean(gen_fake)\n            \n        opt_gen.zero_grad()\n        scalar_gen.scale(loss_gen).backward()\n        scalar_gen.step(opt_gen)\n        scalar_gen.update()\n        \n        alpha += cur_batch_size / (len(dataset) * PROGRESSIVE_EPOCHS[step]*0.5)\n        alpha = min(alpha, 1)\n        \n        if batch_idx % 500 == 0:\n            with torch.no_grad():\n                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n            plot_to_wandb(\n                loss_critic.item(),\n                loss_gen.item(),\n                real.detach(),\n                fixed_fakes.detach()\n            )\n            \n    return alpha","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:52.137368Z","iopub.execute_input":"2022-02-17T19:24:52.137611Z","iopub.status.idle":"2022-02-17T19:24:52.153806Z","shell.execute_reply.started":"2022-02-17T19:24:52.13757Z","shell.execute_reply":"2022-02-17T19:24:52.152836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    gen = Generator(\n        Z_DIM, IN_CHANNELS, CHANNELS_IMG\n    ).to(DEVICE)\n    critic = Discriminator(\n        IN_CHANNELS, CHANNELS_IMG\n    ).to(DEVICE)\n    \n    # initialize optimizers and scalars\n    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n    opt_critic = optim.Adam(\n        critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n    )\n    scalar_critic = torch.cuda.amp.GradScaler()\n    scalar_gen = torch.cuda.amp.GradScaler()\n    \n    if LOAD_MODEL:\n        load_checkpoint(\n            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n        )\n        load_checkpoint(\n            CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,\n        )\n        \n    gen.train()\n    critic.train()\n    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n        alpha = 1e-5\n        loader, dataset = get_loader(4*2**step)\n        print(f\"Image size: {4*2**step}\")\n        \n        for epoch in range(num_epochs):\n            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n            alpha = train_fn(\n                critic,\n                gen,\n                loader,\n                dataset,\n                step,\n                alpha,\n                opt_critic,\n                opt_gen,\n                scalar_gen,\n                scalar_critic\n            )\n            \n            if SAVE_MODEL:\n                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n                \n        step += 1 # progress to the next img size\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:52.822763Z","iopub.execute_input":"2022-02-17T19:24:52.823009Z","iopub.status.idle":"2022-02-17T19:24:52.834184Z","shell.execute_reply.started":"2022-02-17T19:24:52.82298Z","shell.execute_reply":"2022-02-17T19:24:52.833273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:24:58.269166Z","iopub.execute_input":"2022-02-17T19:24:58.269438Z","iopub.status.idle":"2022-02-17T22:17:02.221449Z","shell.execute_reply.started":"2022-02-17T19:24:58.269405Z","shell.execute_reply":"2022-02-17T22:17:02.219438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}